{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b641470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c1c8f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"Test Spark\").config(\"spark.some.config.option\", \"some-value\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0adf0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70947e2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-HF2TGGL.Home:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Test Spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x24690b9a100>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e2fd604",
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eeb29472",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aae7b6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------------------------------------------------------------------+\n",
      "|label|features                                                              |\n",
      "+-----+----------------------------------------------------------------------+\n",
      "|19   |[2.0,8.0,3.0,5.0,1.0,8.0,13.0,0.0,6.0,6.0,10.0,8.0,0.0,8.0,0.0,8.0]   |\n",
      "|8    |[5.0,12.0,3.0,7.0,2.0,10.0,5.0,5.0,4.0,13.0,3.0,9.0,2.0,8.0,4.0,10.0] |\n",
      "|3    |[4.0,11.0,6.0,8.0,6.0,10.0,6.0,2.0,6.0,10.0,3.0,7.0,3.0,7.0,3.0,9.0]  |\n",
      "|13   |[7.0,11.0,6.0,6.0,3.0,5.0,9.0,4.0,6.0,4.0,4.0,10.0,6.0,10.0,2.0,8.0]  |\n",
      "|6    |[2.0,1.0,3.0,1.0,1.0,8.0,6.0,6.0,6.0,6.0,5.0,9.0,1.0,7.0,5.0,10.0]    |\n",
      "|18   |[4.0,11.0,5.0,8.0,3.0,8.0,8.0,6.0,9.0,5.0,6.0,6.0,0.0,8.0,9.0,7.0]    |\n",
      "|1    |[4.0,2.0,5.0,4.0,4.0,8.0,7.0,6.0,6.0,7.0,6.0,6.0,2.0,8.0,7.0,10.0]    |\n",
      "|0    |[1.0,1.0,3.0,2.0,1.0,8.0,2.0,2.0,2.0,8.0,2.0,8.0,1.0,6.0,2.0,7.0]     |\n",
      "|9    |[2.0,2.0,4.0,4.0,2.0,10.0,6.0,2.0,6.0,12.0,4.0,8.0,1.0,6.0,1.0,7.0]   |\n",
      "|12   |[11.0,15.0,13.0,9.0,7.0,13.0,2.0,6.0,2.0,12.0,1.0,9.0,8.0,1.0,1.0,8.0]|\n",
      "+-----+----------------------------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "letter_recognition_df = sqlcontext.read.format('com.databricks.spark.csv').options(header = 'true', inferschema = 'true').load('letter-recognition.csv')\n",
    "feature_columns = ['x-box','y-box','width','high','onpix','x-bar','y-bar','x2bar','y2bar','xybar','x2ybr','xy2br','x-ege','xegvy','y-ege','yegvx']\n",
    "vector_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'features')\n",
    "vectorised_df = vector_assembler.transform(letter_recognition_df).withColumnRenamed('lettr', 'label').select('label', 'features')\n",
    "vectorised_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "15c79ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14928, 5072)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = vectorised_df.randomSplit([0.75, 0.25], seed=12345)\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02ac7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [16, 8, 4, 26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b23d7508",
   "metadata": {},
   "outputs": [],
   "source": [
    "multilayer_perceptron_classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
    "multilayer_perceptron_classifier_model = multilayer_perceptron_classifier.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b12c2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: \n",
      "+-----+--------------------+--------------------+----------+\n",
      "|label|            features|         probability|prediction|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "|    0|[1.0,0.0,2.0,0.0,...|[0.15368389851585...|       0.0|\n",
      "|    0|[1.0,1.0,2.0,1.0,...|[0.15585546934780...|       0.0|\n",
      "|    0|[1.0,1.0,2.0,1.0,...|[0.15642191086169...|       0.0|\n",
      "|    0|[1.0,1.0,2.0,1.0,...|[0.15659746860594...|       0.0|\n",
      "|    0|[1.0,1.0,3.0,2.0,...|[0.15699066639209...|       0.0|\n",
      "|    0|[1.0,3.0,2.0,2.0,...|[0.15808115024074...|       0.0|\n",
      "|    0|[1.0,3.0,2.0,2.0,...|[0.15712097716961...|       0.0|\n",
      "|    0|[2.0,1.0,3.0,2.0,...|[0.15698174241012...|       0.0|\n",
      "|    0|[2.0,1.0,4.0,2.0,...|[0.15692234526692...|       0.0|\n",
      "|    0|[2.0,2.0,4.0,4.0,...|[0.15699595194503...|       0.0|\n",
      "|    0|[2.0,3.0,3.0,1.0,...|[0.15697626056547...|       0.0|\n",
      "|    0|[2.0,3.0,3.0,1.0,...|[0.15702609312631...|       0.0|\n",
      "|    0|[2.0,3.0,4.0,2.0,...|[0.15698146607890...|       0.0|\n",
      "|    0|[2.0,3.0,4.0,2.0,...|[0.15698274472616...|       0.0|\n",
      "|    0|[2.0,3.0,4.0,4.0,...|[0.15698096482224...|       0.0|\n",
      "|    0|[2.0,4.0,3.0,2.0,...|[0.15698149493886...|       0.0|\n",
      "|    0|[2.0,4.0,3.0,3.0,...|[0.15698744721575...|       0.0|\n",
      "|    0|[2.0,4.0,3.0,3.0,...|[0.09804888306103...|       4.0|\n",
      "|    0|[2.0,4.0,3.0,3.0,...|[0.15861159221577...|       0.0|\n",
      "|    0|[2.0,4.0,3.0,3.0,...|[0.15698321927658...|       0.0|\n",
      "+-----+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_predictions_df = multilayer_perceptron_classifier_model.transform(test_df)\n",
    "print(\"TEST DATASET PREDICTIONS AGAINST ACTUAL LABEL: \")\n",
    "test_predictions_df.select(\"label\", \"features\", \"probability\", \"prediction\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "85837ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Test Dataset = 0.252169\n",
      "Precision on Test Dataset = 0.222466\n",
      "Recall on Test Dataset = 0.252169\n"
     ]
    }
   ],
   "source": [
    "prediction_and_labels = test_predictions_df.select(\"prediction\", \"label\")\n",
    "accuracy_evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "precision_evaluator = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\")\n",
    "recall_evaluator = MulticlassClassificationEvaluator(metricName=\"weightedRecall\")\n",
    "print(\"Accuracy on Test Dataset = %g\" % accuracy_evaluator.evaluate(prediction_and_labels))\n",
    "print(\"Precision on Test Dataset = %g\" % precision_evaluator.evaluate(prediction_and_labels))\n",
    "print(\"Recall on Test Dataset = %g\" % recall_evaluator.evaluate(prediction_and_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4d87dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Accuracy on Test Dataset = 0.660686\n"
     ]
    }
   ],
   "source": [
    "new_layers = [16, 16, 12, 26]\n",
    "new_multilayer_perceptron_classifier = MultilayerPerceptronClassifier(maxIter=400, layers=new_layers, blockSize=128, seed=1234)\n",
    "new_multilayer_perceptron_classifier_model = new_multilayer_perceptron_classifier.fit(train_df)\n",
    "new_test_predictions_df = new_multilayer_perceptron_classifier_model.transform(test_df)\n",
    "print(\"New Accuracy on Test Dataset = %g\" % accuracy_evaluator.evaluate(new_test_predictions_df.select(\"prediction\", \"label\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c0dae75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sqlcontext.read.csv(path='letter-recognition.csv', header = True, inferSchema = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c69f03af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------+\n",
      "|label|features             |\n",
      "+-----+---------------------+\n",
      "|0    |[908.813,594.0,702.0]|\n",
      "|0    |[678.0,759.0,633.0]  |\n",
      "|0    |[763.0,866.0,720.0]  |\n",
      "|0    |[719.0,672.0,672.0]  |\n",
      "|0    |[1025.0,867.0,966.0] |\n",
      "|0    |[972.0,932.0,962.0]  |\n",
      "|0    |[539.0,482.0,509.0]  |\n",
      "|0    |[863.0,763.0,818.0]  |\n",
      "|0    |[618.0,677.0,579.0]  |\n",
      "|0    |[678.0,686.0,631.0]  |\n",
      "+-----+---------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_columns = ['Merit Score','SSC','TerminalDegree']\n",
    "vector_assembler = VectorAssembler(inputCols = feature_columns, outputCol = 'features')\n",
    "vectorised_df = vector_assembler.transform(data).withColumnRenamed('Offered', 'label').select('label', 'features')\n",
    "vectorised_df.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8370f614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(497, 158)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df, test_df = vectorised_df.randomSplit([0.75, 0.25], seed=12345)\n",
    "train_df.count(), test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "493df341",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [3, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ab08b32",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o343.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 19) (DESKTOP-HF2TGGL.Home executor driver): org.apache.spark.SparkException: Failed to execute user defined function (OneHotEncoderModel$$Lambda$3514/312775512: (double, int) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\r\n\tat org.apache.spark.storage.BlockManager$$Lambda$2161/1557516830.apply(Unknown Source)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2505/1778459164.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Unseen value: 1.0. To handle unseen values, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1(OneHotEncoder.scala:275)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1$adapted(OneHotEncoder.scala:260)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel$$Lambda$3562/1059849535.apply(Unknown Source)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3625/1140224574.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3623/250584014.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\r\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\r\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\r\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$Lambda$3471/840704509.apply(Unknown Source)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.util.Instrumentation$$$Lambda$3472/2138245732.apply(Unknown Source)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (OneHotEncoderModel$$Lambda$3514/312775512: (double, int) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\r\n\tat org.apache.spark.storage.BlockManager$$Lambda$2161/1557516830.apply(Unknown Source)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2505/1778459164.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen value: 1.0. To handle unseen values, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1(OneHotEncoder.scala:275)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1$adapted(OneHotEncoder.scala:260)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel$$Lambda$3562/1059849535.apply(Unknown Source)\r\n\t... 30 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DRNAUM~1\\AppData\\Local\\Temp/ipykernel_360/1595759489.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmultilayer_perceptron_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultilayerPerceptronClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxIter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblockSize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmultilayer_perceptron_classifier_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultilayer_perceptron_classifier\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\spark3\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             raise TypeError(\n",
      "\u001b[1;32mC:\\spark3\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 379\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    380\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark3\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mJM\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1321\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1323\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark3\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\spark3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o343.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 23.0 failed 1 times, most recent failure: Lost task 0.0 in stage 23.0 (TID 19) (DESKTOP-HF2TGGL.Home executor driver): org.apache.spark.SparkException: Failed to execute user defined function (OneHotEncoderModel$$Lambda$3514/312775512: (double, int) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\r\n\tat org.apache.spark.storage.BlockManager$$Lambda$2161/1557516830.apply(Unknown Source)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2505/1778459164.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Unseen value: 1.0. To handle unseen values, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1(OneHotEncoder.scala:275)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1$adapted(OneHotEncoder.scala:260)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel$$Lambda$3562/1059849535.apply(Unknown Source)\r\n\t... 30 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3625/1140224574.apply(Unknown Source)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$Lambda$3623/250584014.apply(Unknown Source)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\r\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1274)\r\n\tat org.apache.spark.mllib.optimization.LBFGS$.runLBFGS(LBFGS.scala:195)\r\n\tat org.apache.spark.mllib.optimization.LBFGS.optimizeWithLossReturned(LBFGS.scala:154)\r\n\tat org.apache.spark.ml.ann.FeedForwardTrainer.train(Layer.scala:855)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.$anonfun$train$1(MultilayerPerceptronClassifier.scala:228)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier$$Lambda$3471/840704509.apply(Unknown Source)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.util.Instrumentation$$$Lambda$3472/2138245732.apply(Unknown Source)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:184)\r\n\tat org.apache.spark.ml.classification.MultilayerPerceptronClassifier.train(MultilayerPerceptronClassifier.scala:93)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:151)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function (OneHotEncoderModel$$Lambda$3514/312775512: (double, int) => struct<type:tinyint,size:int,indices:array<int>,values:array<double>>)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:177)\r\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1214)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIterator(MemoryStore.scala:223)\r\n\tat org.apache.spark.storage.memory.MemoryStore.putIteratorAsValues(MemoryStore.scala:302)\r\n\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1508)\r\n\tat org.apache.spark.storage.BlockManager$$Lambda$2161/1557516830.apply(Unknown Source)\r\n\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1435)\r\n\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1499)\r\n\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1322)\r\n\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:376)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:327)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$Lambda$2505/1778459164.apply(Unknown Source)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: org.apache.spark.SparkException: Unseen value: 1.0. To handle unseen values, set Param handleInvalid to keep.\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1(OneHotEncoder.scala:275)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel.$anonfun$encoder$1$adapted(OneHotEncoder.scala:260)\r\n\tat org.apache.spark.ml.feature.OneHotEncoderModel$$Lambda$3562/1059849535.apply(Unknown Source)\r\n\t... 30 more\r\n"
     ]
    }
   ],
   "source": [
    "multilayer_perceptron_classifier = MultilayerPerceptronClassifier(maxIter=10, layers=layers, blockSize=12, seed=1234)\n",
    "multilayer_perceptron_classifier_model = multilayer_perceptron_classifier.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3eccad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+----------+------+-----------+--------------+----------------+---+--------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------+-------+\n",
      "|  Application Status|Merit Score|  Domicile|Gender|   Province|Admission Test|Admission Test 2|SSC|TerminalDegree|Optional Subject 1|Optional Subject 2|Optional Subject 3|Optional Subject 4|Optional Subject 5|Document Verified|English Marks|Offered|\n",
      "+--------------------+-----------+----------+------+-----------+--------------+----------------+---+--------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------+-------+\n",
      "|Application Received|    908.813|      OPEN|  male|Baluchistan|             0|               0|594|           702|                 0|                 0|                 0|                 0|                 0|                0|            0|     No|\n",
      "|Application Fee R...|      678.0|      OPEN|  male|       FATA|             0|               0|759|           633|                97|               116|                47|                81|                30|                1|            0|     No|\n",
      "|Application Fee R...|      763.0|Bahawalpur|  male|     Punjab|            48|               0|866|           720|               147|                42|                33|               138|               101|                0|            0|     No|\n",
      "|Application Fee R...|      719.0|Bahawalpur|  male|     Punjab|            52|               0|672|           672|               127|               107|               143|                85|                45|                0|            0|     No|\n",
      "|Application Fee R...|     1025.0|Bahawalpur|female|     Punjab|            71|              71|867|           966|               156|               184|               184|               184|                 0|                0|            0|     No|\n",
      "|Application Fee R...|      972.0|Bahawalpur|  male|     Punjab|            87|              87|932|           962|               173|                42|                48|               184|               179|                1|            0|     No|\n",
      "|Application Fee R...|      539.0|Bahawalpur|  male|     Punjab|            40|               0|482|           509|                69|                86|                93|                34|                 0|                0|            0|     No|\n",
      "|Application Fee R...|      863.0|Bahawalpur|  male|     Punjab|            50|              41|763|           818|               131|               148|               164|               157|                42|                0|            0|     No|\n",
      "|Application Fee R...|      618.0|Bahawalpur|  male|     Punjab|            44|               0|677|           579|                77|                43|                89|                95|               107|                0|            0|     No|\n",
      "|Application Fee R...|      678.0|Bahawalpur|  male|     Punjab|            52|               0|686|           631|                93|                93|                99|               132|                42|                0|            0|     No|\n",
      "|Admission Fee Cha...|      628.0|Bahawalpur|female|     Punjab|            50|               0|521|           588|                81|               102|               106|                46|                84|                1|            0|    Yes|\n",
      "|Application Fee R...|      643.0|Bahawalpur|  male|     Punjab|            47|               0|727|           601|               132|               119|                78|                71|                39|                0|            0|     No|\n",
      "|Admission Fee Cha...|      965.0|Bahawalpur|  male|     Punjab|            67|               0|862|           903|               161|               179|               146|                45|                 0|                1|            0|    Yes|\n",
      "|Admission Fee Cha...|      843.0|Bahawalpur|  male|     Punjab|            43|               0|889|           805|                 0|                 0|                 0|                 0|                 0|                1|            0|    Yes|\n",
      "|Application Fee R...|      692.0|Bahawalpur|  male|     Punjab|            44|               0|741|           653|               108|               113|               106|                40|               101|                0|            0|     No|\n",
      "|Application Fee R...|      650.0|Bahawalpur|  male|     Punjab|            54|               0|778|           601|                71|               111|                98|                42|               111|                0|            0|     No|\n",
      "|Admission Fee Cha...|      983.0|Bahawalpur|  male|     Punjab|            66|               0|861|           922|               177|               155|               154|               181|                46|                1|            0|    Yes|\n",
      "|Admission Fee Cha...|      609.0|Bahawalpur|  male|     Punjab|            53|               0|557|           561|               117|                27|                35|               103|                96|                1|            0|    Yes|\n",
      "|Admission Fee Cha...|      691.0|Bahawalpur|female|     Punjab|            41|               0|779|           655|               118|                80|               135|                85|                95|                1|            0|    Yes|\n",
      "|Application Fee R...|      726.0|Bahawalpur|  male|     Punjab|            62|               0|794|           674|               166|               106|               118|                 0|                41|                0|            0|     No|\n",
      "+--------------------+-----------+----------+------+-----------+--------------+----------------+---+--------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de29a233",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 =data.select(data.TerminalDegree,data.SSC.alias('label'))\n",
    "train, test = data2.randomSplit([0.9,0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bc1796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|features|label|\n",
      "+--------+-----+\n",
      "|   [0.0]|    0|\n",
      "| [481.0]|  508|\n",
      "| [483.0]|  652|\n",
      "| [483.0]|  657|\n",
      "| [487.0]|  686|\n",
      "+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembler=VectorAssembler().setInputCols(['TerminalDegree',]).setOutputCol('features')\n",
    "train01 = assembler.transform(train)\n",
    "train02 = train01.select(\"features\",\"label\")\n",
    "train02.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04ec277b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "model = lr.fit(train02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bcc1693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+-----------------+\n",
      "|features|label|       prediction|\n",
      "+--------+-----+-----------------+\n",
      "| [520.0]|  581|638.6554922323418|\n",
      "| [538.0]|  809|649.8554412567817|\n",
      "| [547.0]|  564|655.4554157690015|\n",
      "| [552.0]|  631|658.5665127202348|\n",
      "| [561.0]|  624|664.1664872324546|\n",
      "+--------+-----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test01 = assembler.transform(test)\n",
    "test02 = test01.select('features', 'label')\n",
    "test03 = model.transform(test02)\n",
    "test03.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7e7b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import polyfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bbc81fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x=data.TerminalDegree\n",
    "#y=train02.select('label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8e1e88f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "\n    Invalid value of type 'pyspark.sql.column.Column' received for the 'x' property of scatter\n        Received value: Column<'TerminalDegree'>\n\n    The 'x' property is an array that may be specified as a tuple,\n    list, numpy array, or pandas Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DRNAUM~1\\AppData\\Local\\Temp/ipykernel_13400/1518503925.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m fig.add_trace(\n\u001b[1;32m----> 5\u001b[1;33m     go.Scatter(\n\u001b[0m\u001b[0;32m      6\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\annaconda1\\lib\\site-packages\\plotly\\graph_objs\\_scatter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, arg, alignmentgroup, cliponaxis, connectgaps, customdata, customdatasrc, dx, dy, error_x, error_y, fill, fillcolor, fillpattern, groupnorm, hoverinfo, hoverinfosrc, hoverlabel, hoveron, hovertemplate, hovertemplatesrc, hovertext, hovertextsrc, ids, idssrc, legendgroup, legendgrouptitle, legendrank, legendwidth, line, marker, meta, metasrc, mode, name, offsetgroup, opacity, orientation, selected, selectedpoints, showlegend, stackgaps, stackgroup, stream, text, textfont, textposition, textpositionsrc, textsrc, texttemplate, texttemplatesrc, uid, uirevision, unselected, visible, x, x0, xaxis, xcalendar, xhoverformat, xperiod, xperiod0, xperiodalignment, xsrc, y, y0, yaxis, ycalendar, yhoverformat, yperiod, yperiod0, yperiodalignment, ysrc, **kwargs)\u001b[0m\n\u001b[0;32m   3433\u001b[0m         \u001b[0m_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3434\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_v\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3435\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"x\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3436\u001b[0m         \u001b[0m_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"x0\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3437\u001b[0m         \u001b[0m_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\annaconda1\\lib\\site-packages\\plotly\\basedatatypes.py\u001b[0m in \u001b[0;36m__setitem__\u001b[1;34m(self, prop, value)\u001b[0m\n\u001b[0;32m   4849\u001b[0m                 \u001b[1;31m# ### Handle simple property ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4850\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4851\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_set_prop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4852\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4853\u001b[0m                 \u001b[1;31m# Make sure properties dict is initialized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\annaconda1\\lib\\site-packages\\plotly\\basedatatypes.py\u001b[0m in \u001b[0;36m_set_prop\u001b[1;34m(self, prop, val)\u001b[0m\n\u001b[0;32m   5193\u001b[0m                 \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5194\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5195\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5197\u001b[0m         \u001b[1;31m# val is None\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\annaconda1\\lib\\site-packages\\plotly\\basedatatypes.py\u001b[0m in \u001b[0;36m_set_prop\u001b[1;34m(self, prop, val)\u001b[0m\n\u001b[0;32m   5188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5189\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5190\u001b[1;33m             \u001b[0mval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalidator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidate_coerce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5191\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_skip_invalid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\annaconda1\\lib\\site-packages\\_plotly_utils\\basevalidators.py\u001b[0m in \u001b[0;36mvalidate_coerce\u001b[1;34m(self, v)\u001b[0m\n\u001b[0;32m    402\u001b[0m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_scalar_or_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    403\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 404\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_invalid_val\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    405\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\annaconda1\\lib\\site-packages\\_plotly_utils\\basevalidators.py\u001b[0m in \u001b[0;36mraise_invalid_val\u001b[1;34m(self, v, inds)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 \u001b[0mname\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;34m\"[\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    289\u001b[0m             \"\"\"\n\u001b[0;32m    290\u001b[0m     \u001b[0mInvalid\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtyp\u001b[0m\u001b[1;33m}\u001b[0m \u001b[0mreceived\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[1;34m'{name}'\u001b[0m \u001b[0mproperty\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mpname\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: \n    Invalid value of type 'pyspark.sql.column.Column' received for the 'x' property of scatter\n        Received value: Column<'TerminalDegree'>\n\n    The 'x' property is an array that may be specified as a tuple,\n    list, numpy array, or pandas Series"
     ]
    }
   ],
   "source": [
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objects as go\n",
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y,\n",
    "        mode='markers',\n",
    "        name='Original_Test',\n",
    "    ))\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=x,\n",
    "        y=y_pred,\n",
    "        name='Predicted'\n",
    "    ))\n",
    "fig.update_layout(\n",
    "    title=\"Linear Regression\",\n",
    "    xaxis_title=\"Displacement\",\n",
    "    yaxis_title=\"Horse Power\",\n",
    "    font=dict(\n",
    "        family=\"Courier New, monospace\",\n",
    "        size=18,\n",
    "        color=\"#7f7f7f\"\n",
    "    )\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3eae22eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32144533540896403\n",
      "8277.846726944514\n",
      "90.9826726742214\n",
      "71.50585015787924\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator()\n",
    "print(evaluator.evaluate(test03,\n",
    "{evaluator.metricName: \"r2\"})\n",
    ")\n",
    "print(evaluator.evaluate(test03,\n",
    "{evaluator.metricName: \"mse\"})\n",
    ")\n",
    "print(evaluator.evaluate(test03,\n",
    "{evaluator.metricName: \"rmse\"})\n",
    ")\n",
    "print(evaluator.evaluate(test03,\n",
    "{evaluator.metricName: \"mae\"})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2240feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testErr = labelsAndPredictions.filter(lambda (v, p): v != p).count() / float(testData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "611305f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned classification tree model:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'LinearRegressionModel' object has no attribute 'toDebugString'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DRNAUM~1\\AppData\\Local\\Temp/ipykernel_13400/309640724.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#print('Test Error = ' + str(testErr))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Learned classification tree model:'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoDebugString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'LinearRegressionModel' object has no attribute 'toDebugString'"
     ]
    }
   ],
   "source": [
    "#print('Test Error = ' + str(testErr))\n",
    "print('Learned classification tree model:')\n",
    "print(model.toDebugString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5086e65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
